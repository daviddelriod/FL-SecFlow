{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of Secure Multiparty Computation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud.\n",
    "- https://ai.googleblog.com/2017/04/federated-learning-collaborative.html\n",
    "- https://www.youtube.com/watch?v=89BGjQYA0uE Desde el 13:50 Hasta 19:26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi,\n",
    "You recently answered me in the OpenFL githhub with 3 examples from your repos. I want to connect with you as I see your profile very interesting and with great knowledge about FL. I would like to connect with you as I am developing my university thesis and I think I could learn a lot from you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import glob\n",
    "import torchvision\n",
    "#import cv2\n",
    "import tqdm as tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import openfl.native as fx\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from openfl.interface.interactive_api.federation import Federation\n",
    "from openfl.interface.interactive_api.experiment import TaskInterface, DataInterface, ModelInterface, FLExperiment\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'api'\n",
    "cert_dir = 'cert'\n",
    "director_node_fqdn = 'localhost'\n",
    "director_port = '50051'\n",
    "\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=director_port, \n",
    "    tls=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['150', '150', '1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federation.target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 150, 1)\n",
      "(150, 150, 1)\n"
     ]
    }
   ],
   "source": [
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=2)\n",
    "dummy_shard_dataset = dummy_shard_desc.get_dataset('train')\n",
    "sample, target = dummy_shard_dataset[0]\n",
    "print(sample.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformedDataset(Dataset):\n",
    "    \"\"\"Image Person ReID Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, transform=None, target_transform=None):\n",
    "        \"\"\"Initialize Dataset.\"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Length of dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, label = self.dataset[index]\n",
    "        label = self.target_transform(label) if self.target_transform else label\n",
    "        img = self.transform(img) if self.transform else img\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(DataInterface):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "        \n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor  will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        self.train_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('train'),\n",
    "            transform=None\n",
    "        )\n",
    "        self.valid_set = TransformedDataset(\n",
    "            self._shard_descriptor.get_dataset('val'),\n",
    "            transform=None\n",
    "        )\n",
    "        \n",
    "    def get_train_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks with optimizer in contract\n",
    "        \"\"\"\n",
    "        generator=torch.Generator()\n",
    "        generator.manual_seed(0)\n",
    "        return DataLoader(\n",
    "            self.train_set, batch_size=self.kwargs['train_bs'], shuffle=True, generator=generator\n",
    "            )\n",
    "\n",
    "    def get_valid_loader(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Output of this method will be provided to tasks without optimizer in contract\n",
    "        \"\"\"\n",
    "        return DataLoader(self.valid_set, batch_size=self.kwargs['valid_bs'])\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.train_set)\n",
    "\n",
    "    def get_valid_data_size(self):\n",
    "        \"\"\"\n",
    "        Information for aggregation\n",
    "        \"\"\"\n",
    "        return len(self.valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = ChestXrayDataset(train_bs=32, valid_bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=2, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(12800, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(p=0.25) # Add dropout layer\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x) # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)#probar tanH\n",
    "        return x\n",
    "\n",
    "model_net = Net()\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for param in model_net.parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "'''\n",
    "FEDPROX\n",
    "'''        \n",
    "#from openfl.utilities.optimizers.torch import FedProxAdam        \n",
    "#optimizer = FedProxAdam(params_to_update, lr=1e-4, mu=0.01)\n",
    "\n",
    "'''\n",
    "ORIGINALE\n",
    "'''\n",
    "optimizer = optim.Adam(params_to_update, lr=1e-3)\n",
    "#optimizer = optim.AdamW(params_to_update, lr=0.001, weight_decay=0.02)\n",
    "#optimizer = optim.SGD(params_to_update, lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "#scheduler\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "def binary_cross_entropy(output, target):\n",
    "    \"\"\"Cross-entropy metric\n",
    "    \"\"\"\n",
    "    #return F.cross_entropy(input=output,target=target)\n",
    "    #return F.binary_cross_entropy_with_logits(input=output,target=target)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    loss = criterion(output, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_adapter = 'openfl.plugins.frameworks_adapters.pytorch_adapter.FrameworkAdapterPlugin'\n",
    "model_interface = ModelInterface(model=model_net, optimizer=optimizer, framework_plugin=framework_adapter)\n",
    "\n",
    "# Save the initial model state\n",
    "initial_model = deepcopy(model_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (4014948915.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\clash\\AppData\\Local\\Temp\\ipykernel_16640\\4014948915.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    #@task_interface.set_aggregation_function(FedCurvWeightedAverage())\u001b[0m\n\u001b[1;37m                                                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "task_interface = TaskInterface()\n",
    "\n",
    "# The Interactive API supports registering functions definied in main module or imported.\n",
    "def function_defined_in_notebook(some_parameter):\n",
    "    print(f'Also I accept a parameter and it is {some_parameter}')\n",
    "\n",
    "# Task interface currently supports only standalone functions.\n",
    "@task_interface.add_kwargs(**{'some_parameter': 42})\n",
    "@task_interface.register_fl_task(model='net_model', data_loader='train_loader',\n",
    "                     device='device', optimizer='optimizer') \n",
    "#@task_interface.set_aggregation_function(FedCurvWeightedAverage()\n",
    "def train(net_model, train_loader, optimizer, device, loss_fn=binary_cross_entropy, some_parameter=None):\n",
    "    torch.manual_seed(0)\n",
    "    #fedcurv.on_train_begin(net_model)\n",
    "    device='cuda'\n",
    "    function_defined_in_notebook(some_parameter)\n",
    "    \n",
    "    train_loader = tqdm.tqdm(train_loader, desc=\"train\")\n",
    "    net_model.train()\n",
    "    net_model.to(device)\n",
    "\n",
    "    losses = []\n",
    "    epochs = 50\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = torch.tensor(data).to(device), torch.tensor(\n",
    "                target).to(device, dtype=torch.int64)\n",
    "            optimizer.zero_grad()\n",
    "            #data = data.type(torch.LongTensor)\n",
    "            #target = target.type(torch.LongTensor)\n",
    "            output = net_model(data)\n",
    "            #output = output.logits #per GOOGLENET\n",
    "            loss = loss_fn(output=output, target=target) #+ fedcurv.get_penalty(net_model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "    #fedcurv.on_train_end(net_model, train_loader, device)    \n",
    "    return {'train_loss': np.mean(losses),}\n",
    "\n",
    "@task_interface.register_fl_task(model='net_model', data_loader='val_loader', device='device')     \n",
    "def validate(net_model, val_loader, device):\n",
    "    torch.manual_seed(0)\n",
    "    device = torch.device('cuda')\n",
    "    net_model.eval()\n",
    "    net_model.to(device)\n",
    "    \n",
    "    val_loader = tqdm.tqdm(val_loader, desc=\"validate\")\n",
    "    val_score = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            samples = target.shape[0]\n",
    "            total_samples += samples\n",
    "            data, target = torch.tensor(data).to(device), \\\n",
    "                torch.tensor(target).to(device, dtype=torch.int64)\n",
    "            output = net_model(data)\n",
    "            #da wine\n",
    "            #_, preds = torch.max(outputs, dim=1)\n",
    "            #return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "            \n",
    "            #originale\n",
    "            #pred = output.argmax(dim=1,keepdim=True)\n",
    "            \n",
    "            #tentativo\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            val_score += pred.eq(target).sum().cpu().numpy()\n",
    "            \n",
    "    return {'acc': val_score / total_samples,}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'ChestXray_EPOCHS50_ROUND20_CNN'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our dataset and model to perform federated learning on. The dataset should be composed of a numpy arrayWe start with a simple fully connected model that is trained on the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Workspace Directories\n",
      "Creating Workspace Templates\n",
      "Successfully installed packages from C:\\Users\\clash\\.local\\workspace/requirements.txt.\n",
      "\n",
      "New workspace directory structure:\n",
      "workspace\n",
      "├── .workspace\n",
      "├── agg_to_col_one_signed_cert.zip\n",
      "├── agg_to_col_two_signed_cert.zip\n",
      "├── cert\n",
      "├── data\n",
      "│   └── MNIST\n",
      "│       ├── processed\n",
      "│       └── raw\n",
      "├── director.yaml\n",
      "├── envoy_config.yaml\n",
      "├── final_pytorch_model\n",
      "├── lightning_logs\n",
      "│   ├── version_0\n",
      "│   │   ├── events.out.tfevents.1674498751.LAPTOP-S11BJ6U7.9884.0\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_1\n",
      "│   │   ├── events.out.tfevents.1674498850.LAPTOP-S11BJ6U7.9884.1\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_10\n",
      "│   │   ├── events.out.tfevents.1674503091.LAPTOP-S11BJ6U7.9884.10\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_11\n",
      "│   │   ├── events.out.tfevents.1674503246.LAPTOP-S11BJ6U7.9884.11\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_12\n",
      "│   │   ├── events.out.tfevents.1674503343.LAPTOP-S11BJ6U7.9884.12\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_13\n",
      "│   │   ├── events.out.tfevents.1674503535.LAPTOP-S11BJ6U7.9884.13\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_14\n",
      "│   │   ├── events.out.tfevents.1674503716.LAPTOP-S11BJ6U7.9884.14\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_15\n",
      "│   │   ├── events.out.tfevents.1674503820.LAPTOP-S11BJ6U7.9884.15\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_16\n",
      "│   │   ├── events.out.tfevents.1674503910.LAPTOP-S11BJ6U7.9884.16\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_17\n",
      "│   │   ├── events.out.tfevents.1674503916.LAPTOP-S11BJ6U7.9884.17\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_18\n",
      "│   │   ├── events.out.tfevents.1674504538.LAPTOP-S11BJ6U7.9884.18\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_19\n",
      "│   │   ├── events.out.tfevents.1674504566.LAPTOP-S11BJ6U7.9884.19\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_2\n",
      "│   │   ├── events.out.tfevents.1674499041.LAPTOP-S11BJ6U7.9884.2\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_20\n",
      "│   │   ├── events.out.tfevents.1674504886.LAPTOP-S11BJ6U7.9884.20\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_21\n",
      "│   │   ├── events.out.tfevents.1674505054.LAPTOP-S11BJ6U7.9884.21\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_22\n",
      "│   │   ├── events.out.tfevents.1674514627.LAPTOP-S11BJ6U7.9884.22\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_23\n",
      "│   │   ├── events.out.tfevents.1674515073.LAPTOP-S11BJ6U7.9884.23\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_24\n",
      "│   │   ├── events.out.tfevents.1674515143.LAPTOP-S11BJ6U7.9884.24\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_25\n",
      "│   │   ├── events.out.tfevents.1674515239.LAPTOP-S11BJ6U7.9884.25\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_26\n",
      "│   │   ├── events.out.tfevents.1674515310.LAPTOP-S11BJ6U7.9884.26\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_27\n",
      "│   │   ├── events.out.tfevents.1674515382.LAPTOP-S11BJ6U7.9884.27\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_28\n",
      "│   │   ├── checkpoints\n",
      "│   │   ├── events.out.tfevents.1674515453.LAPTOP-S11BJ6U7.9884.28\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_29\n",
      "│   │   ├── events.out.tfevents.1674516275.LAPTOP-S11BJ6U7.9884.29\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_3\n",
      "│   │   ├── events.out.tfevents.1674499114.LAPTOP-S11BJ6U7.9884.3\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_30\n",
      "│   │   ├── checkpoints\n",
      "│   │   ├── events.out.tfevents.1674516387.LAPTOP-S11BJ6U7.9884.30\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_31\n",
      "│   │   ├── events.out.tfevents.1674517827.LAPTOP-S11BJ6U7.9884.31\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_32\n",
      "│   │   ├── events.out.tfevents.1674518069.LAPTOP-S11BJ6U7.9884.32\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_33\n",
      "│   │   ├── events.out.tfevents.1674520930.LAPTOP-S11BJ6U7.9884.33\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_34\n",
      "│   │   ├── events.out.tfevents.1674521253.LAPTOP-S11BJ6U7.9884.34\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_35\n",
      "│   │   ├── events.out.tfevents.1674521831.LAPTOP-S11BJ6U7.9884.35\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_4\n",
      "│   │   ├── events.out.tfevents.1674499281.LAPTOP-S11BJ6U7.9884.4\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_5\n",
      "│   │   ├── events.out.tfevents.1674499776.LAPTOP-S11BJ6U7.9884.5\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_6\n",
      "│   │   ├── checkpoints\n",
      "│   │   ├── events.out.tfevents.1674499922.LAPTOP-S11BJ6U7.9884.6\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_7\n",
      "│   │   ├── events.out.tfevents.1674502167.LAPTOP-S11BJ6U7.9884.7\n",
      "│   │   └── hparams.yaml\n",
      "│   ├── version_8\n",
      "│   │   ├── events.out.tfevents.1674502840.LAPTOP-S11BJ6U7.9884.8\n",
      "│   │   └── hparams.yaml\n",
      "│   └── version_9\n",
      "│       ├── events.out.tfevents.1674503063.LAPTOP-S11BJ6U7.9884.9\n",
      "│       └── hparams.yaml\n",
      "├── logs\n",
      "│   ├── cnn_mnist\n",
      "│   │   ├── events.out.tfevents.1672864004.LAPTOP-S11BJ6U7.15700.0\n",
      "│   │   ├── events.out.tfevents.1674409165.LAPTOP-S11BJ6U7.4092.0\n",
      "│   │   ├── events.out.tfevents.1674417749.LAPTOP-S11BJ6U7.11908.0\n",
      "│   │   ├── events.out.tfevents.1674432594.LAPTOP-S11BJ6U7.11132.0\n",
      "│   │   ├── events.out.tfevents.1674432666.LAPTOP-S11BJ6U7.11132.1\n",
      "│   │   ├── events.out.tfevents.1674432819.LAPTOP-S11BJ6U7.11132.2\n",
      "│   │   ├── events.out.tfevents.1674581560.LAPTOP-S11BJ6U7.6932.0\n",
      "│   │   ├── events.out.tfevents.1674681993.LAPTOP-S11BJ6U7.18032.0\n",
      "│   │   ├── events.out.tfevents.1674682837.LAPTOP-S11BJ6U7.10356.0\n",
      "│   │   ├── events.out.tfevents.1674683052.LAPTOP-S11BJ6U7.18032.1\n",
      "│   │   ├── events.out.tfevents.1674684020.LAPTOP-S11BJ6U7.18032.2\n",
      "│   │   ├── events.out.tfevents.1674684936.LAPTOP-S11BJ6U7.18032.3\n",
      "│   │   ├── events.out.tfevents.1674685187.LAPTOP-S11BJ6U7.18032.4\n",
      "│   │   ├── events.out.tfevents.1674685362.LAPTOP-S11BJ6U7.18032.5\n",
      "│   │   ├── events.out.tfevents.1674685407.LAPTOP-S11BJ6U7.18032.6\n",
      "│   │   ├── events.out.tfevents.1674687021.LAPTOP-S11BJ6U7.10356.1\n",
      "│   │   └── events.out.tfevents.1674688644.LAPTOP-S11BJ6U7.14252.1\n",
      "│   └── tensorboard\n",
      "│       ├── events.out.tfevents.1672864089.LAPTOP-S11BJ6U7\n",
      "│       ├── events.out.tfevents.1674417764.LAPTOP-S11BJ6U7\n",
      "│       └── events.out.tfevents.1674581583.LAPTOP-S11BJ6U7\n",
      "├── plan\n",
      "│   ├── cols.yaml\n",
      "│   ├── data.yaml\n",
      "│   ├── defaults\n",
      "│   └── plan.yaml\n",
      "├── requirements.txt\n",
      "├── save\n",
      "│   ├── init.pbuf\n",
      "│   ├── torch_cnn_mnist_best.pbuf\n",
      "│   ├── torch_cnn_mnist_init.pbuf\n",
      "│   └── torch_cnn_mnist_last.pbuf\n",
      "├── shard_descriptor.py\n",
      "├── spam_metric.log\n",
      "└── src\n",
      "    ├── mnist_utils.py\n",
      "    ├── ptmnist_inmemory.py\n",
      "    ├── pt_cnn.py\n",
      "    ├── __init__.py\n",
      "    └── __pycache__\n",
      "        ├── mnist_utils.cpython-37.pyc\n",
      "        └── __init__.cpython-37.pyc\n",
      "\n",
      "52 directories, 115 files\n",
      "Setting Up Certificate Authority...\n",
      "\n",
      "1.  Create Root CA\n",
      "1.1 Create Directories\n",
      "1.2 Create Database\n",
      "1.3 Create CA Request and Certificate\n",
      "2.  Create Signing Certificate\n",
      "2.1 Create Directories\n",
      "2.2 Create Database\n",
      "2.3 Create Signing Certificate CSR\n",
      "2.4 Sign Signing Certificate CSR\n",
      "3   Create Certificate Chain\n",
      "\n",
      "Done.\n",
      "Creating AGGREGATOR certificate key pair with following settings: CN=\u001b[31mlaptop-s11bj6u7\u001b[0m, SAN=\u001b[31mDNS:laptop-s11bj6u7\u001b[0m\n",
      "  Writing AGGREGATOR certificate key pair to: \u001b[32mc:\\Users\\clash\\Desktop\\4\\TFG\\cert/server\u001b[0m\n",
      "The CSR Hash for file \u001b[32mserver/agg_laptop-s11bj6u7.csr\u001b[0m = \u001b[31m0b6be613902f9ce4740cc2716afc0a0ae1284271be77824a8a7edd8d2c21530aa8758f23cb9ce705d9193ce195f3cb3d\u001b[0m\n",
      " Signing AGGREGATOR certificate\n",
      "Creating COLLABORATOR certificate key pair with following settings: CN=\u001b[31mone\u001b[0m, SAN=\u001b[31mDNS:one\u001b[0m\n",
      "  Moving COLLABORATOR certificate to: \u001b[32mc:\\Users\\clash\\Desktop\\4\\TFG\\cert/col_one\u001b[0m\n",
      "The CSR Hash for file \u001b[32mcol_one.csr\u001b[0m = \u001b[31mcc80ada79afd2f1cdd7cdbf63d05c81d962fe35534b8d008b41cbae5cfaa56231c3c8731059586bb26cb953d9bec2497\u001b[0m\n",
      " Signing COLLABORATOR certificate\n",
      "\n",
      "Registering \u001b[32msers\\clash\\Desktop\\4\\TFG\\cert\\client\\col_one\u001b[0m in \u001b[32mC:\\Users\\clash\\.local\\workspace\\plan\\cols.yaml\u001b[0m\n",
      "Creating COLLABORATOR certificate key pair with following settings: CN=\u001b[31mtwo\u001b[0m, SAN=\u001b[31mDNS:two\u001b[0m\n",
      "  Moving COLLABORATOR certificate to: \u001b[32mc:\\Users\\clash\\Desktop\\4\\TFG\\cert/col_two\u001b[0m\n",
      "The CSR Hash for file \u001b[32mcol_two.csr\u001b[0m = \u001b[31mb82bb25319981f53fc3a09db79b71187b3eedbf7929eb0509e2e6855d76ed89fdcfcd5770dd39574a1e35c0d5d377cea\u001b[0m\n",
      " Signing COLLABORATOR certificate\n",
      "\n",
      "Registering \u001b[32msers\\clash\\Desktop\\4\\TFG\\cert\\client\\col_two\u001b[0m in \u001b[32mC:\\Users\\clash\\.local\\workspace\\plan\\cols.yaml\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "fl_experiment.start(\n",
    "    model_provider=model_interface, \n",
    "    task_keeper=task_interface,\n",
    "    data_loader=fed_dataset,\n",
    "    rounds_to_train=20,\n",
    "    opt_treatment='CONTINUE_GLOBAL'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d69f63ea15b6bb008b8d31206bc050e6263a4c89b8d4a8f3080901fb7da99ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
